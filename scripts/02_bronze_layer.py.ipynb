{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d263ee70-7201-4c39-845d-30a1059c50a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Definições do Unity Catalog\n",
    "catalog_name = \"workspace\"\n",
    "landing_schema = \"landing_db\"\n",
    "landing_volume = \"landing_files\"\n",
    "bronze_schema = \"bronze_db\"\n",
    "\n",
    "# Caminho base para os arquivos CSV no Volume\n",
    "volume_path_base = f\"/Volumes/{catalog_name}/{landing_schema}/{landing_volume}\"\n",
    "\n",
    "# Garante que o schema de destino exista\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{bronze_schema}\")\n",
    "\n",
    "tables = [\n",
    "    \"companies\", \"company_reviews\", \"employment_types\", \"industries\",\n",
    "    \"job_benefits\", \"job_skills\", \"jobs\", \"locations\", \"salary_ranges\", \"skills\"\n",
    "]\n",
    "\n",
    "erros_bronze = []\n",
    "\n",
    "for table_name in tables:\n",
    "    print(f\"Processando para camada Bronze: {table_name}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Monta o caminho para a pasta onde os CSVs foram salvos pelo script anterior\n",
    "        landing_csv_path = f\"{volume_path_base}/{table_name}\"\n",
    "        \n",
    "        # Lê os arquivos CSV do Volume\n",
    "        try:\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(landing_csv_path)\n",
    "        except Exception as e_read:\n",
    "            raise Exception(f\"Falha ao ler arquivos em {landing_csv_path}. Verifique se a ingestão funcionou. Detalhe: {e_read}\")\n",
    "        \n",
    "        df_with_metadata = df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "        # Salva como uma tabela Delta gerenciada no schema Bronze do Catalog\n",
    "        full_table_name = f\"{catalog_name}.{bronze_schema}.{table_name}\"\n",
    "        df_with_metadata.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_table_name)\n",
    "        \n",
    "        print(f\"Tabela {full_table_name} salva com sucesso no Catalog.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        msg_erro = f\"ERRO ao processar tabela {table_name}: {str(e)[0:300]}...\"\n",
    "        print(msg_erro)\n",
    "        erros_bronze.append(msg_erro)\n",
    "\n",
    "if len(erros_bronze) > 0:\n",
    "    print(\"\\n---------------------------------------------------\")\n",
    "    print(f\"O processo Bronze terminou com {len(erros_bronze)} erros:\")\n",
    "    for erro in erros_bronze:\n",
    "        print(erro)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    raise Exception(\"Falha no processamento da camada Bronze. Verifique os logs.\")\n",
    "else:\n",
    "    print(\"\\nProcesso da camada Bronze finalizado com SUCESSO TOTAL.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_layer.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
