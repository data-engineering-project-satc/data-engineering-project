{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf96656-65fc-4973-9272-bf874f998e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id, year, month, dayofmonth, quarter, date_format, lit, current_timestamp\n",
    "\n",
    "# Definição dos schemas do Unity Catalog\n",
    "silver_schema = \"workspace.silver_db\"\n",
    "gold_schema = \"workspace.gold_db\"\n",
    "\n",
    "# Garante que o schema de destino exista\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_schema}\")\n",
    "\n",
    "erros_gold = []\n",
    "                                       \n",
    "# --- CRIAR DIMENSÕES ---\n",
    "\n",
    "# 1. dim_companies\n",
    "try:\n",
    "    print(\"Criando dim_companies...\")\n",
    "    # Lendo tabelas necessárias\n",
    "    df_companies = spark.read.table(f\"{silver_schema}.companies\")\n",
    "    df_industries = spark.read.table(f\"{silver_schema}.industries\")\n",
    "\n",
    "    dim_companies = df_companies.join(df_industries, df_companies.industry_id == df_industries.id, \"left\") \\\n",
    "        .select(\n",
    "            col(\"companies.id\").alias(\"company_id\"),\n",
    "            col(\"company_name\"),\n",
    "            col(\"company_rating\"),\n",
    "            col(\"industry_name\")\n",
    "        )\n",
    "\n",
    "    # Colunas SCD2 (Snapshot Strategy)\n",
    "    dim_companies = dim_companies.withColumn(\"is_current\", lit(True)) \\\n",
    "                                 .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                                 .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    dim_companies.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.dim_companies\")\n",
    "    print(\"SUCESSO: dim_companies criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_companies: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 2. dim_locations\n",
    "try:\n",
    "    print(\"Criando dim_locations...\")\n",
    "    df_locations = spark.read.table(f\"{silver_schema}.locations\")\n",
    "\n",
    "    dim_locations = df_locations.select(\n",
    "        col(\"id\").alias(\"location_id\"), \"city\", \"state_abbr\"\n",
    "    )\n",
    "\n",
    "    dim_locations = dim_locations.withColumn(\"is_current\", lit(True)) \\\n",
    "                                 .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                                 .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    dim_locations.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.dim_locations\")\n",
    "    print(\"SUCESSO: dim_locations criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_locations: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 3. dim_skills\n",
    "try:\n",
    "    print(\"Criando dim_skills...\")\n",
    "    df_skills = spark.read.table(f\"{silver_schema}.skills\")\n",
    "\n",
    "    dim_skills = df_skills.select(col(\"id\").alias(\"skill_id\"), col(\"skill_name\"))\n",
    "\n",
    "    dim_skills = dim_skills.withColumn(\"is_current\", lit(True)) \\\n",
    "                           .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                           .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "                           \n",
    "    dim_skills.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.dim_skills\")\n",
    "    print(\"SUCESSO: dim_skills criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_skills: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 4. dim_date\n",
    "try:\n",
    "    print(\"Criando dim_date...\")\n",
    "    df_jobs = spark.read.table(f\"{silver_schema}.jobs\") # Data vem de jobs\n",
    "\n",
    "    dim_date = df_jobs.select(col(\"listing_date\").alias(\"date\")).distinct() \\\n",
    "        .withColumn(\"date_id\", monotonically_increasing_id()) \\\n",
    "        .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "        .withColumn(\"quarter\", quarter(col(\"date\"))) \\\n",
    "        .withColumn(\"month_name\", date_format(col(\"date\"), \"MMMM\")) \\\n",
    "        .select(\"date_id\", \"date\", \"year\", \"month\", \"day\", \"quarter\", \"month_name\")\n",
    "    \n",
    "    dim_date.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.dim_date\")\n",
    "    print(\"SUCESSO: dim_date criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_date: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# --- CRIAR TABELAS FATO ---\n",
    "\n",
    "# Tabela Fato principal: fact_jobs\n",
    "try:\n",
    "    print(\"Criando fact_jobs...\")\n",
    "    # Recarregar Dataframes necessários para garantir contexto\n",
    "    df_jobs = spark.read.table(f\"{silver_schema}.jobs\")\n",
    "    df_salary = spark.read.table(f\"{silver_schema}.salary_ranges\")\n",
    "    # Ler dim_date recém criada ou existente\n",
    "    dim_date = spark.read.table(f\"{gold_schema}.dim_date\")\n",
    "\n",
    "    fact_jobs = df_jobs.join(dim_date, df_jobs.listing_date == dim_date.date, \"inner\") \\\n",
    "        .join(df_salary, df_jobs.salary_range_id == df_salary.id, \"left\") \\\n",
    "        .select(\n",
    "            col(\"jobs.id\").alias(\"job_id\"),\n",
    "            col(\"date_id\"),\n",
    "            col(\"jobs.company_id\"),\n",
    "            col(\"jobs.location_id\"),\n",
    "            col(\"jobs.employment_type_id\"),\n",
    "            col(\"job_title\"),\n",
    "            col(\"min_salary\"),\n",
    "            col(\"max_salary\"),\n",
    "            col(\"avg_salary\")\n",
    "        )\n",
    "    \n",
    "    fact_jobs.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.fact_jobs\")\n",
    "    print(\"SUCESSO: fact_jobs criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar fact_jobs: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# Tabela Fato de ponte: fact_job_skills\n",
    "try:\n",
    "    print(\"Criando fact_job_skills...\")\n",
    "    df_job_skills = spark.read.table(f\"{silver_schema}.job_skills\")\n",
    "\n",
    "    fact_job_skills = df_job_skills.select(col(\"job_id\"), col(\"skill_id\"))\n",
    "    fact_job_skills.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.fact_job_skills\")\n",
    "    print(\"SUCESSO: fact_job_skills criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar fact_job_skills: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "print(\"\\n---------------------------------------------------\")\n",
    "if len(erros_gold) > 0:\n",
    "    print(f\"O processo Gold terminou com {len(erros_gold)} erros:\")\n",
    "    for erro in erros_gold:\n",
    "        print(erro)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    raise Exception(\"Falha no processamento da camada Gold. Verifique os logs acima.\")\n",
    "else:\n",
    "    print(\"Processo da camada Gold finalizado com SUCESSO TOTAL.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_gold_layer.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
