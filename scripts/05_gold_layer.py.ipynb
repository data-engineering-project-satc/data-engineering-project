{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf96656-65fc-4973-9272-bf874f998e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id, year, month, dayofmonth, quarter, date_format, lit, current_timestamp, sha2, concat_ws, when, max as spark_max\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Definição dos schemas do Unity Catalog\n",
    "silver_schema = \"workspace.silver_db\"\n",
    "gold_schema = \"workspace.gold_db\"\n",
    "\n",
    "# Garante que o schema de destino exista\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_schema}\")\n",
    "\n",
    "erros_gold = []\n",
    "\n",
    "# FUNÇÃO 1: SCD TIPO 2 (Para as Dimensões)\n",
    "def executar_scd2(spark, df_source, target_table_full_name, key_columns, exclude_cols=[]):\n",
    "    # 1. PREPARAÇÃO: Calcula o HASH na origem\n",
    "    cols_to_hash = [c for c in df_source.columns if c not in key_columns and c not in exclude_cols]\n",
    "    df_source_hashed = df_source.withColumn(\"row_hash\", sha2(concat_ws(\"||\", *cols_to_hash), 256))\n",
    "\n",
    "    # 2. Cria tabela se não existir (Carga Inicial)\n",
    "    try:\n",
    "        spark.read.table(target_table_full_name).limit(1).count()\n",
    "\n",
    "        if \"row_hash\" not in df_target.columns:\n",
    "            raise Exception(\"Schema Incompatível - Recriando tabela\")\n",
    "    except:\n",
    "        print(f\"Dimensão {target_table_full_name} não existe. Criando carga inicial...\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {target_table_full_name}\")\n",
    "        df_init = df_source.withColumn(\"is_current\", lit(True)) \\\n",
    "                           .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                           .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "        df_init.write.format(\"delta\").saveAsTable(target_table_full_name)\n",
    "        return\n",
    "\n",
    "    # 3. Lógica de Merge SCD2\n",
    "    delta_target = DeltaTable.forName(spark, target_table_full_name)\n",
    "    cols_to_hash = [c for c in df_source.columns if c not in key_columns and c not in exclude_cols]\n",
    "    \n",
    "    df_source_hashed = df_source.withColumn(\"row_hash\", sha2(concat_ws(\"||\", *cols_to_hash), 256))\n",
    "    \n",
    "    df_target_active = spark.read.table(target_table_full_name).filter(\"is_current = true\") \\\n",
    "        .select(*key_columns, \"row_hash\") \\\n",
    "        .withColumnRenamed(\"row_hash\", \"target_hash\")\n",
    "\n",
    "    join_cond = [df_source_hashed[k] == df_target_active[k] for k in key_columns]\n",
    "    \n",
    "    df_staged = df_source_hashed.join(df_target_active, join_cond, \"left\") \\\n",
    "        .withColumn(\"action\", \n",
    "            when(col(\"target_hash\").isNull(), \"INSERT\")\n",
    "            .when(col(\"row_hash\") != col(\"target_hash\"), \"UPDATE\")\n",
    "            .otherwise(\"NO_ACTION\")\n",
    "        ).filter(\"action != 'NO_ACTION'\")\n",
    "\n",
    "    if df_staged.count() == 0:\n",
    "        print(f\"Nenhuma alteração para {target_table_full_name}.\")\n",
    "        return\n",
    "\n",
    "    df_updates = df_staged.filter(\"action = 'UPDATE'\").select(*key_columns, lit(\"true\").alias(\"mergeKey\"))\n",
    "    \n",
    "    df_inserts = df_staged.select(*df_source.columns, lit(None).alias(\"mergeKey\")) \\\n",
    "        .withColumn(\"is_current\", lit(True)) \\\n",
    "        .withColumn(\"start_date\", current_timestamp()) \\\n",
    "        .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    for c in df_inserts.columns:\n",
    "        if c not in df_updates.columns:\n",
    "            df_updates = df_updates.withColumn(c, lit(None))\n",
    "            \n",
    "    df_merge_source = df_inserts.unionByName(df_updates, allowMissingColumns=True)\n",
    "\n",
    "    merge_condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in key_columns])\n",
    "    \n",
    "    delta_target.alias(\"target\").merge(\n",
    "        df_merge_source.alias(\"source\"),\n",
    "        f\"{merge_condition} AND (source.mergeKey = 'true')\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set = {\"is_current\": \"false\", \"end_date\": \"current_timestamp()\"}\n",
    "    ).whenNotMatchedInsert(\n",
    "        values = {c: f\"source.{c}\" for c in df_source.columns if c != \"mergeKey\"} | \n",
    "                 {\"is_current\": \"true\", \"start_date\": \"current_timestamp()\", \"end_date\": \"null\"}\n",
    "    ).execute()\n",
    "    print(f\"SCD2 executado com sucesso em {target_table_full_name}\")\n",
    "                                       \n",
    "# --- CRIAR DIMENSÕES ---\n",
    "\n",
    "# 1. dim_companies\n",
    "try:\n",
    "    print(\"Criando dim_companies...\")\n",
    "    # Lendo tabelas necessárias\n",
    "    df_companies = spark.read.table(f\"{silver_schema}.companies\")\n",
    "    df_industries = spark.read.table(f\"{silver_schema}.industries\")\n",
    "\n",
    "    dim_companies = df_companies.join(df_industries, df_companies.industry_id == df_industries.id, \"left\") \\\n",
    "        .select(\n",
    "            col(\"companies.id\").alias(\"company_id\"),\n",
    "            col(\"company_name\"),\n",
    "            col(\"company_rating\"),\n",
    "            col(\"industry_name\")\n",
    "        )\n",
    "\n",
    "    executar_scd2(spark, dim_companies, f\"{gold_schema}.dim_companies\", [\"company_id\"])\n",
    "    print(\"SUCESSO: dim_companies criada.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_companies: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 2. dim_locations\n",
    "try:\n",
    "    print(\"Criando dim_locations...\")\n",
    "    df_locations = spark.read.table(f\"{silver_schema}.locations\")\n",
    "\n",
    "    dim_locations = df_locations.select(\n",
    "        col(\"id\").alias(\"location_id\"), \"city\", \"state_abbr\"\n",
    "    )\n",
    "\n",
    "    executar_scd2(spark, dim_locations, f\"{gold_schema}.dim_locations\", [\"location_id\"])\n",
    "    print(\"SUCESSO: dim_locations criada.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_locations: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 3. dim_skills\n",
    "try:\n",
    "    print(\"Criando dim_skills...\")\n",
    "    df_skills = spark.read.table(f\"{silver_schema}.skills\")\n",
    "\n",
    "    dim_skills = df_skills.select(col(\"id\").alias(\"skill_id\"), col(\"skill_name\"))\n",
    "\n",
    "    executar_scd2(spark, dim_skills, f\"{gold_schema}.dim_skills\", [\"skill_id\"])\n",
    "    print(\"SUCESSO: dim_skills criada.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_skills: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# 4. dim_date\n",
    "try:\n",
    "    print(\"Criando dim_date...\")\n",
    "    df_jobs = spark.read.table(f\"{silver_schema}.jobs\") # Data vem de jobs\n",
    "\n",
    "    dim_date = df_jobs.select(col(\"listing_date\").alias(\"date\")).distinct() \\\n",
    "        .withColumn(\"date_id\", monotonically_increasing_id()) \\\n",
    "        .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "        .withColumn(\"quarter\", quarter(col(\"date\"))) \\\n",
    "        .withColumn(\"month_name\", date_format(col(\"date\"), \"MMMM\")) \\\n",
    "        .select(\"date_id\", \"date\", \"year\", \"month\", \"day\", \"quarter\", \"month_name\")\n",
    "    \n",
    "    dim_date.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.dim_date\")\n",
    "    print(\"SUCESSO: dim_date criada.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar dim_date: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# --- CRIAR TABELAS FATO ---\n",
    "\n",
    "# Tabela Fato principal: fact_jobs\n",
    "try:\n",
    "    print(\"Processando fact_jobs (Incremental)...\")\n",
    "    target_fact = f\"{gold_schema}.fact_jobs\"\n",
    "    \n",
    "    # 1. Determinar o Checkpoint (Data máxima já processada)\n",
    "    try:\n",
    "        max_date_row = spark.read.table(target_fact).agg(spark_max(\"listing_date\").alias(\"max_date\")).collect()[0]\n",
    "        last_checkpoint = max_date_row[\"max_date\"]\n",
    "    except:\n",
    "        last_checkpoint = None\n",
    "    \n",
    "    print(f\"Último checkpoint (Data Máxima na Gold): {last_checkpoint}\")\n",
    "\n",
    "    # 2. Ler dados da Silver\n",
    "    df_jobs = spark.read.table(f\"{silver_schema}.jobs\")\n",
    "    \n",
    "    # 3. Filtrar apenas dados novos (Incremental)\n",
    "    if last_checkpoint:\n",
    "        print(f\"Filtrando dados posteriores a {last_checkpoint}...\")\n",
    "        df_jobs_new = df_jobs.filter(col(\"listing_date\") > last_checkpoint)\n",
    "    else:\n",
    "        print(\"Carga Inicial (Full Load)...\")\n",
    "        df_jobs_new = df_jobs\n",
    "\n",
    "    # Se não tem dados novos, pula\n",
    "    if df_jobs_new.count() > 0:\n",
    "        # Preparar dados\n",
    "        df_salary = spark.read.table(f\"{silver_schema}.salary_ranges\")\n",
    "        dim_date = spark.read.table(f\"{gold_schema}.dim_date\")\n",
    "\n",
    "        fact_jobs = df_jobs_new.join(dim_date, df_jobs_new.listing_date == dim_date.date, \"inner\") \\\n",
    "            .join(df_salary, df_jobs_new.salary_range_id == df_salary.id, \"left\") \\\n",
    "            .select(\n",
    "                col(\"jobs.id\").alias(\"job_id\"),\n",
    "                col(\"date_id\"),\n",
    "                col(\"jobs.listing_date\"),\n",
    "                col(\"jobs.company_id\"),\n",
    "                col(\"jobs.location_id\"),\n",
    "                col(\"jobs.employment_type_id\"),\n",
    "                col(\"job_title\"),\n",
    "                col(\"min_salary\"),\n",
    "                col(\"max_salary\"),\n",
    "                col(\"avg_salary\")\n",
    "            )\n",
    "        \n",
    "        # Gravar Incrementalmente (APPEND)\n",
    "        fact_jobs.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(target_fact)\n",
    "        print(f\"SUCESSO: {fact_jobs.count()} novos registros inseridos em fact_jobs.\")\n",
    "    else:\n",
    "        print(\"Nenhum dado novo para fact_jobs.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO fact_jobs: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "# Tabela Fato de ponte: fact_job_skills\n",
    "try:\n",
    "    print(\"Criando fact_job_skills...\")\n",
    "    df_job_skills = spark.read.table(f\"{silver_schema}.job_skills\")\n",
    "\n",
    "    fact_job_skills = df_job_skills.select(col(\"job_id\"), col(\"skill_id\"))\n",
    "    fact_job_skills.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_schema}.fact_job_skills\")\n",
    "    print(\"SUCESSO: fact_job_skills criada.\")\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO ao criar fact_job_skills: {e}\"\n",
    "    print(msg)\n",
    "    erros_gold.append(msg)\n",
    "\n",
    "print(\"\\n---------------------------------------------------\")\n",
    "if len(erros_gold) > 0:\n",
    "    print(f\"O processo Gold terminou com {len(erros_gold)} erros:\")\n",
    "    for erro in erros_gold:\n",
    "        print(erro)\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    raise Exception(\"Falha no processamento da camada Gold. Verifique os logs acima.\")\n",
    "else:\n",
    "    print(\"Processo da camada Gold finalizado com SUCESSO TOTAL.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_gold_layer.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
