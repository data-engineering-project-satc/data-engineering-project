{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a8beea-7c79-4680-a077-37f549c028b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, split, to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, DecimalType\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# --- DEFINIÇÃO DOS CAMINHOS ---\n",
    "bronze_base_path = \"/mnt/datalake/bronze/\"\n",
    "silver_base_path = \"/mnt/datalake/silver/\"\n",
    "\n",
    "tables_to_process = [\n",
    "    \"companies\", \"company_reviews\", \"employment_types\", \"industries\",\n",
    "    \"job_benefits\", \"job_skills\", \"jobs\", \"locations\", \"salary_ranges\", \"skills\"\n",
    "]\n",
    "\n",
    "print(\"Iniciando processo da camada Silver...\")\n",
    "\n",
    "# --- TRANSFORMAÇÕES ---\n",
    "\n",
    "# 1. Tabela 'salary_ranges': Limpar e extrair valores numéricos de salário\n",
    "try:\n",
    "    print(\"Processando 'salary_ranges'...\")\n",
    "    df_salary_bronze = spark.read.format(\"delta\").load(f\"{bronze_base_path}salary_ranges\")\n",
    "\n",
    "    df_salary_silver = df_salary_bronze \\\n",
    "        .withColumn(\"range_cleaned\", regexp_replace(col(\"range_description\"), \"[\\\\$kK]\", \"\")) \\\n",
    "        .withColumn(\"salary_parts\", split(col(\"range_cleaned\"), \"-\")) \\\n",
    "        .withColumn(\"min_salary\", (col(\"salary_parts\").getItem(0)).cast(IntegerType) * 1000) \\\n",
    "        .withColumn(\"max_salary\", (col(\"salary_parts\").getItem(1)).cast(IntegerType) * 1000) \\\n",
    "        .withColumn(\"avg_salary\", (col(\"min_salary\") + col(\"max_salary\")) / 2) \\\n",
    "        .select(\"id\", \"range_description\", \"min_salary\", \"max_salary\", \"avg_salary\")\n",
    "\n",
    "    df_salary_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}salary_ranges\")\n",
    "    print(\"'salary_ranges' processada com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar 'salary_ranges': {e}\")\n",
    "\n",
    "\n",
    "# 2. Tabela 'jobs': Converter data e garantir tipos corretos\n",
    "try:\n",
    "    print(\"Processando 'jobs'...\")\n",
    "    df_jobs_bronze = spark.read.format(\"delta\").load(f\"{bronze_base_path}jobs\")\n",
    "    \n",
    "    df_jobs_silver = df_jobs_bronze \\\n",
    "        .withColumn(\"listing_date\", to_timestamp(col(\"listing_date\"))) \\\n",
    "        .withColumn(\"job_description\", col(\"job_description\").cast(\"string\")) # Garantir tipo\n",
    "        \n",
    "    df_jobs_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}jobs\")\n",
    "    print(\"'jobs' processada com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar 'jobs': {e}\")\n",
    "\n",
    "# 3. Tabela 'companies': Garantir tipo decimal para o rating\n",
    "try:\n",
    "    print(\"Processando 'companies'...\")\n",
    "    df_companies_bronze = spark.read.format(\"delta\").load(f\"{bronze_base_path}companies\")\n",
    "    \n",
    "    df_companies_silver = df_companies_bronze \\\n",
    "        .withColumn(\"company_rating\", col(\"company_rating\").cast(DecimalType(3, 1)))\n",
    "\n",
    "    df_companies_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}companies\")\n",
    "    print(\"'companies' processada com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar 'companies': {e}\")\n",
    "\n",
    "\n",
    "# 4. Outras tabelas dimensionais (sem transformações complexas nesta fase)\n",
    "# Apenas movemos de Bronze para Silver, garantindo o formato Delta.\n",
    "simple_tables = [\"company_reviews\", \"employment_types\", \"industries\", \"job_benefits\", \"job_skills\", \"locations\", \"skills\"]\n",
    "for table_name in simple_tables:\n",
    "    try:\n",
    "        print(f\"Processando '{table_name}'...\")\n",
    "        df_bronze = spark.read.format(\"delta\").load(f\"{bronze_base_path}{table_name}\")\n",
    "        df_bronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_base_path}{table_name}\")\n",
    "        print(f\"'{table_name}' processada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar '{table_name}': {e}\")\n",
    "\n",
    "print(\"Processo da camada Silver finalizado.\")\n",
    "\n",
    "\n",
    "# --- EXEMPLO DE LÓGICA SCD TIPO 2 (PARA FUTURAS CARGAS) ---\n",
    "\"\"\"\n",
    "# Esta lógica não deve ser executada na primeira carga.\n",
    "# Ela é usada para atualizar dimensões que mudam ao longo do tempo.\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "# Suponha que 'df_companies_updates' seja um novo batch de dados da camada Bronze\n",
    "# e 'silver_companies_table' seja a tabela Delta existente na camada Silver.\n",
    "\n",
    "silver_companies_table = DeltaTable.forPath(spark, f\"{silver_base_path}companies\")\n",
    "\n",
    "# Adicione colunas para controle do SCD2\n",
    "df_companies_updates = df_companies_updates.withColumn(\"is_current\", lit(True)) \\\n",
    "                                           .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                                           .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "# Lógica de MERGE para implementar SCD Tipo 2\n",
    "silver_companies_table.alias(\"target\") \\\n",
    "  .merge(\n",
    "    df_companies_updates.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(\n",
    "    condition = \"target.is_current = true AND (target.company_rating <> source.company_rating OR target.industry_id <> source.industry_id)\",\n",
    "    set = {\"is_current\": \"false\", \"end_date\": \"source.start_date\"}\n",
    "  ) \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_layer.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
