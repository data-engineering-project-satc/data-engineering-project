{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ddf18a-d781-4b0d-96b3-735d3cbec145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType\n",
    "\n",
    "print(\"Iniciando a preparação e validação da estrutura da Camada Gold...\")\n",
    "\n",
    "# --- ETAPA 1: DEFINIÇÕES ---\n",
    "catalog_name = \"workspace\"\n",
    "gold_schema_name = \"gold_db\"\n",
    "full_gold_schema = f\"{catalog_name}.{gold_schema_name}\"\n",
    "\n",
    "# Lista das tabelas de dimensão que precisam ter as colunas SCD2\n",
    "dimensions_to_check = [\"dim_companies\", \"dim_locations\", \"dim_skills\"] \n",
    "\n",
    "# Nome da tabela de checkpoint para as tabelas fato\n",
    "checkpoint_table_name = \"fact_load_checkpoints\"\n",
    "\n",
    "# --- ETAPA 2: GARANTIR QUE O SCHEMA GOLD EXISTA ---\n",
    "print(f\"Garantindo que o schema '{full_gold_schema}' exista...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {full_gold_schema}\")\n",
    "spark.sql(f\"USE {full_gold_schema}\") # Define o schema como padrão para os comandos seguintes\n",
    "\n",
    "# --- ETAPA 3: VERIFICAR E ADICIONAR COLUNAS SCD2 ÀS DIMENSÕES ---\n",
    "print(\"\\nVerificando a estrutura das tabelas de dimensão para SCD Tipo 2...\")\n",
    "\n",
    "for table_name in dimensions_to_check:\n",
    "    try:\n",
    "        # Pega o schema atual da tabela\n",
    "        current_schema = spark.read.table(table_name).schema\n",
    "        existing_columns = [field.name.lower() for field in current_schema.fields]\n",
    "        \n",
    "        # Colunas que queremos garantir que existam\n",
    "        columns_to_add = {\n",
    "            \"is_current\": \"BOOLEAN\",\n",
    "            \"start_date\": \"TIMESTAMP\",\n",
    "            \"end_date\": \"TIMESTAMP\"\n",
    "        }\n",
    "        \n",
    "        # String para o comando ALTER TABLE\n",
    "        add_cols_sql_list = []\n",
    "        \n",
    "        for col_name, col_type in columns_to_add.items():\n",
    "            if col_name.lower() not in existing_columns:\n",
    "                print(f\"Coluna '{col_name}' não encontrada na tabela '{table_name}'. Adicionando...\")\n",
    "                add_cols_sql_list.append(f\"{col_name} {col_type}\")\n",
    "        \n",
    "        if add_cols_sql_list:\n",
    "            sql_command = f\"ALTER TABLE {table_name} ADD COLUMNS ({', '.join(add_cols_sql_list)})\"\n",
    "            print(f\"Executando: {sql_command}\")\n",
    "            spark.sql(sql_command)\n",
    "            print(f\"Tabela '{table_name}' atualizada com sucesso.\")\n",
    "        else:\n",
    "            print(f\"Tabela '{table_name}' já possui a estrutura SCD2 correta.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Se a tabela não existir, o spark.read.table vai falhar.\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "            print(f\"Aviso: Tabela de dimensão '{table_name}' ainda não existe. Será criada na carga da camada Gold.\")\n",
    "        else:\n",
    "            print(f\"Erro ao verificar a tabela '{table_name}': {e}\")\n",
    "\n",
    "\n",
    "# --- ETAPA 4: GARANTIR QUE A TABELA DE CHECKPOINT EXISTA ---\n",
    "print(\"\\nVerificando a existência da tabela de checkpoint...\")\n",
    "\n",
    "try:\n",
    "    # Tenta descrever a tabela para ver se ela existe\n",
    "    spark.sql(f\"DESCRIBE TABLE {checkpoint_table_name}\")\n",
    "    print(f\"Tabela de checkpoint '{checkpoint_table_name}' já existe.\")\n",
    "except Exception as e:\n",
    "    if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e):\n",
    "        print(f\"Tabela de checkpoint '{checkpoint_table_name}' não encontrada. Criando agora...\")\n",
    "        \n",
    "        # Define o schema da tabela de checkpoint\n",
    "        checkpoint_schema = StructType([\n",
    "            StructField(\"table_name\", StringType(), True),\n",
    "            StructField(\"last_processed_timestamp\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Cria um DataFrame vazio com o schema correto\n",
    "        empty_df = spark.createDataFrame([], schema=checkpoint_schema)\n",
    "        \n",
    "        # Salva o DataFrame vazio como uma tabela Delta, criando a tabela\n",
    "        empty_df.write.format(\"delta\").saveAsTable(checkpoint_table_name)\n",
    "        \n",
    "        print(f\"Tabela de checkpoint '{checkpoint_table_name}' criada com sucesso.\")\n",
    "    else:\n",
    "        print(f\"Erro ao verificar a tabela de checkpoint: {e}\")\n",
    "\n",
    "print(\"\\nPreparação da Camada Gold finalizada. A estrutura está pronta para a carga incremental.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_prepare_to_gold.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
