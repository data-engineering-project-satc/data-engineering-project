{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434fd8e0-cdb7-49c6-98f9-6f49e6c2e4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install psycopg2-binary\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "try:\n",
    "    script_dir = os.getcwd()\n",
    "    repo_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n",
    "    dotenv_path = os.path.join(repo_root, \".env\")\n",
    "\n",
    "    print(f\"Raiz do repositório detectada em: {repo_root}\")\n",
    "    print(f\"Tentando carregar o arquivo .env de: {dotenv_path}\")\n",
    "\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(\"Arquivo .env carregado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Não foi possível carregar o arquivo .env: {e}\")\n",
    "\n",
    "# Cria o schema e volume se não existir\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"landing_db\"\n",
    "volume_name = \"landing_files\"\n",
    "\n",
    "full_schema_name = f\"{catalog_name}.{schema_name}\"\n",
    "full_volume_name = f\"{full_schema_name}.{volume_name}\"\n",
    "volume_path_base = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "\n",
    "print(f\"Garantindo que o schema '{full_schema_name}' exista...\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {full_schema_name}\")\n",
    "\n",
    "print(f\"Garantindo que o volume '{full_volume_name}' exista...\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {full_volume_name}\")\n",
    "print(\"Infraestrutura no Unity Catalog verificada/criada com sucesso.\")\n",
    "\n",
    "# --- CONFIGURAÇÃO DA CONEXÃO COM POSTGRESQL (SUPABASE) ---\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Validação para garantir que as variáveis foram carregadas\n",
    "if not all([db_user, db_password, db_host, db_port, db_name]):\n",
    "    raise ValueError(\n",
    "        \"Uma ou mais variáveis de ambiente do banco de dados não foram definidas. Verifique o arquivo .env\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Credenciais do banco de dados carregadas com sucesso!\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}?sslmode=require&sslrootcert=system\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "# --- PROCESSO DE INGESTÃO ---\n",
    "\n",
    "# Lista de tabelas para ingestão\n",
    "tables = [\n",
    "    \"companies\",\n",
    "    \"company_reviews\",\n",
    "    \"employment_types\",\n",
    "    \"industries\",\n",
    "    \"job_benefits\",\n",
    "    \"job_skills\",\n",
    "    \"jobs\",\n",
    "    \"locations\",\n",
    "    \"salary_ranges\",\n",
    "    \"skills\",\n",
    "]\n",
    "\n",
    "# Loop para ler cada tabela e salvar na camada Landing como CSV\n",
    "for table_name in tables:\n",
    "    print(f\"Iniciando ingestão da tabela: {table_name}\")\n",
    "\n",
    "    try:\n",
    "        # Leitura da tabela do banco de dados\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url, table=f\"public.{table_name}\", properties=connection_properties\n",
    "        )\n",
    "\n",
    "        # Define o caminho de destino na camada Landing\n",
    "        landing_path = f\"{volume_path_base}/{table_name}\"\n",
    "\n",
    "        # Salva os dados como CSV\n",
    "        df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\n",
    "            landing_path\n",
    "        )\n",
    "\n",
    "        print(f\"Tabela {table_name} salva em {landing_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao ingerir a tabela {table_name}: {e}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion_landing.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
